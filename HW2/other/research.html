
<html>
<head>
<link type="text/css" rel="stylesheet" media="all" href="style1.css" />


</head>

<body>

<table width="1300" border="2">
<tr>
<td colspan="5" style="background-color:#FFFFFF;">
<h1 style="font-size:50; font-family:Lucida Console, Monaco, monospace;">ResearcH</h1>
</td>
</tr>
<tr>
<td style="background-color:#FFFFFF;width:100px;text-align:top;">
<b><a href="index.html">HomE</a></b><br />
</td>
<td style="background-color:#FFD7F0;width:100px;text-align:top;">
<b><a href="research.html">ResearcH</a></b><br />
</td>
<td style="background-color:#FF000A;width:100px;text-align:top;">
<b><a href="education.html">EducatioN</a></b><br />
</td>
<td style="background-color:#FFA700;width:100px;text-align:top;">
<b><a href="extra.html">Extra-CurriculaR</a></b><br />
</td>
<td style="background-color:#0FD700;width:100px;text-align:top;">
<b><a href="contact.html">ContacT</a></b><br />
</td>
<td style="background-color:#0ABCDE;width:100px;text-align:top;">
<b><a href="blog.html">BloG</a></b><br />
</td>
<td style="background-color:#FFFFFF;width:100px;text-align:top; color:#000000">
<b><a href="CV_rohan.shah.pdf">ResumE</a></b><br />
</td>
</tr>
</table>

<table width="1300" border="2">
<tr>
<td colspan="3">
<a style="color:#000000; font-size:17;">
My research focusses on Machine Learning and Computer Vision. These are areas that I am intrigued by, not only because of how intellectually challenging they are- but also because of the innumerable possibilities that they have the potential to unearth, in the next half-decade.
<br/>
<br/>
Needless to say, research in these fields go hand-in-hand with a lot of experience in programming, algorithm design, and real-time systems, among other computer science essentials.  
<br/>
I currently hold a computer science Research Assistantship position at the GRASP lab @UPenn, under Dr. Ben Taskar.
</td>
</tr>

<tr>
<td colspan="4">
<a style="color:#000000; font-size:15;">
<b>
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</b>
</b>
</td>
</tr>

<tr>
<td colspan="1">
<img src="gait.jpg" alt="Pulpit rock" width="300" height="300" />
</td>
<td colspan="1">
<a style="color:#000000; font-size:15;">
<b>
Human Pose Estimation for Gait Analysis<br/>
</b>
[May 2012 - Present]
<br/>
My current research with Dr. Ben Taskar and Ben Sapp is based on using machine learning and computer vision based techniques for Human Pose Estimation- in order detect important features of gait analysis. For this we are using a Kinect to record different gaits so that we can use its depth and RGB streams to perform pose estimation on the resulting point-cloud data. The pose estimation would basically consist of a learning algorithm that estimates in detail the human pose in a video using Pictorial Structures (i.e. by representing the human body as a structure made of a finite number of connected components.)
<br/>
<br/>

</a>
</td>
<td colspan="1">
<img src="gait3.jpg" alt="Pulpit rock" width="300" height="300" />
</td>

</tr>
<tr>
<td colspan="4">
<a style="color:#000000; font-size:15;">
<b>
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</b>
</td>
</tr>

<tr>
<td>
<a style="color:#000000; font-size:15;">
<b>
Smart Cameras for Sensor Tracking
<br/>
</b>
[Oct 2011 - April 2012]
<br/>
Under Dr. Camillo Jose Taylor, this research work in the GRASP lab was concerned with the use of a network of low-cost, easily-deployable smart cameras in a scenario where sensors or devices need to be tracked efficiently. <br/>
The set-up consists of a network of multiple "smart" cameras that recognize certain pre-specified blink codes and communicate to each other wirelessly, thus forming a distributed tracking system, and a blinker circuit which I designed that would be recognized by these cameras.<br/>
The main objective of my research was to establish the correct parameters of a projective transform, or Homography, between the ground plane and the image plane of each camera, so as to facilitate effective sensor tracking when this blinker was placed on a mobile robot that would go through the viewing space of the cameras.<br/>
</a>
</td>
<td colspan="1">
<img src="smartcam2.jpg" alt="Pulpit rock" width="400" height="300" />
</td>
<td colspan="1">
<img src="smartcam1.jpg" alt="Pulpit rock" width="300" height="300" />
</td>
<td colspan="1">
<img src="smartcam3.jpg" alt="Pulpit rock" width="300" height="300" />
</td>

</tr>
<tr>
<td colspan="4">
<a style="color:#000000; font-size:15;">
<b>
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</b>
</td>
</tr>

<tr>
<td colspan="2">
<img src="gesture1.jpg" alt="Pulpit rock" width="700" height="300" />
</td>
<td>
<a style="color:#000000; font-size:15;">
<b>
Dynamic Hand Gesture Recognition
<br/>
</b>
[May 2010 - July 2011]
<br/>
This work involved the design of a real-time dynamic hand gesture recognition algorithm, implemented to recognize hand gestures using just the video input of a web-camera.<br/> 
The algorithm uses Haar-like features and orientation histograms, and has been implemented using openCV and integrated with graphical applications like "slide-show", where the slides of a presentation could be moved by merely waving the hand, and "virtual drawing", in which one gesture (palm-up) is used to draw and another gesture (palm-sideways) is used to erase in an application similar to Paint. 
The work was demonstrated on an ARM-based Beagleboard, running Linux.<br/> 
</a>
</td>
</tr>
<tr>
<td>
<a style="color:#000000; font-size:15;">
- Ranked 2nd best project of the year by the ECE department<br/>
- Short-listed among top 20 projects at the Indian Institute of Science, Bangalore, India; in the national-level Jed-I Project Challenge(June 2011).<br/>
- The paper written on this work can be found <a href="Under_grad_Gesture.pdf">here</a>.
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

</a>
<br/>
<br/>
</td>
<td colspan="1">
<img src="gesture2.jpg" alt="Pulpit rock" width="400" height="300" />
</td>
</td>
<td colspan="1">
<img src="gesture3.JPG" alt="Pulpit rock" width="300" height="300" />
</tr>
<tr>
<td colspan="4">
<a style="color:#000000; font-size:15;">
<b>
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</b>
</b>
</td>
</tr>
<tr>
<td colspan="1">
<img src="igvc.jpg" alt="Pulpit rock" width="300" height="300" />
</td>
<td>
<a style="color:#000000; font-size:15;">
<b>
Intelligent Ground Vehicle Competition
<br/>
</b>
[October 2009 - June 2010]
<br/>
Participated in the 18th Annual IGVC, held at Oakland University, Rochester, Michigan, from June 4-7, 2010. Involved building an unmanned ground vehicle capable of following a prescribed track while avoiding all obstacles in its path, and also having the functionalities of waypoint navigation (i.e. capable of autonomously using an in-built GPS to get to a prescribed destination co-ordinate). The vehicle was also capable of waypoint navigation using a programmed GARMIN HPS-17 GPS receiver.
<br/>
<br/>
</td>
</tr>



</body>
</html>